{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcc00e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\" \", sep=\",\", encoding=\"cp1252\", engine='python')\n",
    "\n",
    "# Extract the 'content' column\n",
    "a = df[\"content\"]\n",
    "\n",
    "# Cleaning the text\n",
    "a = a.str.replace(r'[^\\x01-\\x7F]', '', regex=True)  # Remove non-ASCII characters\n",
    "a = a.str.replace(r'http\\S+\\s*', '', regex=True)     # Remove URLs\n",
    "a = a.str.replace(r'\\bRT\\b', '', regex=True)         # Remove 'RT'\n",
    "a = a.str.replace(r'#', '', regex=True)              # Remove hashtags\n",
    "a = a.str.replace(r'@\\S+', '', regex=True)           # Remove mentions\n",
    "a = a.str.replace(r'[\\x00-\\x1F\\x7F]', '', regex=True) # Remove control characters\n",
    "a = a.str.replace(r'\\d', '', regex=True)             # Remove digits\n",
    "a = a.str.replace(r'[^\\w\\s]', '', regex=True)        # Remove punctuation\n",
    "a = a.str.replace(r'^\\s*', '', regex=True)           # Remove leading whitespace\n",
    "a = a.str.replace(r'\\s*$', '', regex=True)           # Remove trailing whitespace\n",
    "\n",
    "# Convert to lowercase\n",
    "a = a.str.lower()\n",
    "\n",
    "# Define specific stopwords\n",
    "custom_stopwords = [\"anti flu shot\", \"antiflushot\", \"flu shot\", \"flushot\", \"flu season\", \"fluseason\", \"flu vaccine\", \"fluvaccine\", \"flu vaccination\", \"fluvaccination\", \"influenza vaccine\", \"influenzavaccine\", \"influenza vaccination\", \"influenzavaccination\", \"flu\", \"influenza\"]\n",
    "\n",
    "# Remove specific stopwords\n",
    "def remove_custom_stopwords(text):\n",
    "    return ' '.join(word for word in text.split() if word not in custom_stopwords)\n",
    "\n",
    "a = a.apply(remove_custom_stopwords)\n",
    "\n",
    "# Create a new DataFrame with the cleaned tweets\n",
    "df1 = pd.DataFrame(a, columns=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c0f7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --user numpy scipy gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e850dae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import gensim\n",
    "\n",
    "# Additional preprocessing of the cleaned tweets\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        \n",
    "data = df1.content.values.tolist()\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad04548",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc3f25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords and lemmatize the text\n",
    "import nltk \n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "nltk.download('stopwords')  \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "import spacy \n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags and len(token)>3] )\n",
    "    return texts_out\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "data_lemmatized = lemmatization(data_words_nostops, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']) \n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc43de4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove commas and merge each inner list into a single string\n",
    "cleaned_data = [\" \".join(item).replace(\",\", \"\") for item in data_lemmatized]\n",
    "\n",
    "# Convert the list of cleaned strings to a DataFrame\n",
    "df2 = pd.DataFrame(cleaned_data, columns=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b16e42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "#Create Bag of Words representation of the tweets\n",
    "vectorizer = CountVectorizer(analyzer='word', min_df=10, stop_words='english', lowercase=True, token_pattern='[a-zA-Z0-9]{3,}')\n",
    "data_vectorized = vectorizer.fit_transform(df2[\"content\"])\n",
    "data_dense = data_vectorized.todense()\n",
    "\n",
    "# Compute Sparsicity = Percentage of Non-Zero cells\n",
    "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016518d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "#Calculate AIC scores\n",
    "def get_gmm_labels(data_vectorized, k):\n",
    "    gmm = GaussianMixture(n_components=k, max_iter=200, random_state=37)\n",
    "    gmm.fit(data_vectorized)\n",
    "    aic = gmm.aic(data_vectorized)\n",
    "    print('{}: aic={}'.format(k, aic))\n",
    "    return k, aic\n",
    "data_vectorized = data_vectorized.asfptype()\n",
    "\n",
    "U, S, V = svds(data_vectorized, k=20)\n",
    "gmm_scores_aic = [get_gmm_labels(U, k) for k in range(2, 51)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0db4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "#Calculate BIC scores\n",
    "def get_gmm_labels(data_vectorized, k):\n",
    "    gmm = GaussianMixture(n_components=k, max_iter=200, random_state=37)\n",
    "    gmm.fit(data_vectorized)\n",
    "    bic = gmm.bic(data_vectorized)\n",
    "    print('{}: bic={}'.format(k, bic))\n",
    "    return k, bic\n",
    "data_vectorized = data_vectorized.asfptype()\n",
    "\n",
    "U, S, V = svds(data_vectorized, k=20)\n",
    "gmm_scores_bic = [get_gmm_labels(U, k) for k in range(2, 51)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00185c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot AIC and BIC scores\n",
    "def plot_scores(scores, ax, ylabel):\n",
    "    _x = [s[0] for s in scores]\n",
    "    _y = [s[1] for s in scores]\n",
    "\n",
    "    ax.plot(_x, _y, color='tab:blue')\n",
    "    ax.set_xlabel('k')\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title('{} vs k'.format(ylabel))\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(15, 5))\n",
    "plot_scores(gmm_scores_aic, ax[0], 'GMM AIC')\n",
    "plot_scores(gmm_scores_bic, ax[1], 'GMM BIC')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad3377d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate topics based on the no. of optimal topics which is '7' in this case \n",
    "dictionary = gensim.corpora.Dictionary(data_lemmatized)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in data_lemmatized]\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary,random_state=0)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6b5b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find dominant topic in each sentence of the text\n",
    "def format_topics_sentences(ldamodel=lda_model, corpus=bow_corpus, texts=data_lemmatized):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "     # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=bow_corpus, texts=data_lemmatized)\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8605a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
